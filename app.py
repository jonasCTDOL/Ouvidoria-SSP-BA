import streamlit as st
import mysql.connector
import pandas as pd
from huggingface_hub import InferenceClient

# --- CONFIGURA√á√ÉO DA P√ÅGINA ---
st.set_page_config(
    page_title="An√°lise de Colabora√ß√µes com IA",
    page_icon="üí°"
)

# --- FUN√á√ïES DE L√ìGICA ---

@st.cache_data(ttl=3600)
def fetch_data_from_db():
    """Conecta-se √† base de dados MySQL e busca todos os dados."""
    try:
        conn = mysql.connector.connect(**st.secrets["mysql"])
        query = "SELECT * FROM colaboracoes;"
        df = pd.read_sql(query, conn)
        conn.close()
        # Converte colunas de data para um formato mais leg√≠vel
        if 'created_at' in df.columns:
            df['created_at'] = pd.to_datetime(df['created_at']).dt.date
        return df
    except Exception as e:
        st.error(f"Ocorreu um erro ao conectar-se ou buscar dados: {e}")
        return None

def get_data_summary(df):
    """Cria um resumo estat√≠stico e informativo do DataFrame."""
    summary_parts = []
    
    summary_parts.append(f"Resumo Geral do Conjunto de Dados:")
    summary_parts.append(f"- N√∫mero total de colabora√ß√µes: {len(df)}")
    
    if 'created_at' in df.columns and not df['created_at'].empty:
        summary_parts.append(f"- Per√≠odo dos dados: de {df['created_at'].min()} a {df['created_at'].max()}")
        
    summary_parts.append("\nAn√°lise das Colunas Principais:")
    
    # Descreve as colunas mais importantes de forma leg√≠vel
    for col in ['tipo_colaboracao', 'cidade', 'estado', 'status', 'anonimato']:
        if col in df.columns:
            summary_parts.append(f"\n- Contagem de valores para a coluna '{col}':")
            # Mostra a contagem dos valores mais comuns
            value_counts = df[col].value_counts().to_string()
            summary_parts.append(value_counts)
            
    return "\n".join(summary_parts)

def generate_insight_huggingface(user_question, df):
    """
    Envia um resumo dos dados para a API do Hugging Face para obter uma resposta inteligente.
    """
    candidate_models = [
        "meta-llama/Meta-Llama-3-8B-Instruct",
        "google/gemma-2-9b-it",
        "HuggingFaceH4/zephyr-7b-beta",
        "mistralai/Mixtral-8x7B-Instruct-v0.1"
    ]
    
    try:
        api_token = st.secrets["huggingface_api"]["token"]
        st.info(f"A usar o token que come√ßa com '{api_token[:6]}' e termina com '{api_token[-4:]}'.")
    except Exception as e:
        st.error("Erro ao ler o token da API. Verifique a sec√ß√£o `[huggingface_api]` nos seus 'Secrets'.")
        return None

    # NOVIDADE: Gera um resumo inteligente em vez de usar os dados brutos.
    data_summary = get_data_summary(df)

    system_prompt = """Voc√™ √© um analista de dados de elite. A sua √∫nica fun√ß√£o √© analisar o resumo estat√≠stico que o utilizador fornece e responder √† pergunta dele de forma clara e profissional, em portugu√™s. Baseie a sua resposta **exclusivamente** no resumo. N√£o invente informa√ß√µes."""

    user_prompt = f"""
Aqui est√° um resumo estat√≠stico dos dados de colabora√ß√µes:
--- RESUMO DOS DADOS ---
{data_summary}

--- PERGUNTA DO UTILIZADOR ---
Com base **exclusivamente** no resumo acima, responda √† seguinte pergunta: {user_question}
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    for model_id in candidate_models:
        st.info(f"A testar o modelo de IA: {model_id}...")
        
        try:
            client = InferenceClient(model=model_id, token=api_token)
            response = client.chat_completion(
                messages=messages,
                max_tokens=1024,
                temperature=0.3, # Diminu√≠do para respostas ainda mais factuais
                top_p=0.95
            )
            
            insight = response.choices[0].message.content
            
            st.success(f"Modelo '{model_id}' respondeu com sucesso!")
            return insight.strip()

        except Exception as e:
            error_message = str(e)
            if "401" in error_message:
                 st.error(f"Erro de Autentica√ß√£o (401) com o modelo '{model_id}'. O seu token de API √© inv√°lido.")
                 return None
            else:
                 st.warning(f"O modelo '{model_id}' falhou com um erro: {error_message}. A tentar o pr√≥ximo modelo...")
            continue
    
    st.error("N√£o foi poss√≠vel obter uma resposta de nenhum dos modelos de IA dispon√≠veis. Por favor, tente novamente mais tarde.")
    return None

# --- INTERFACE DO UTILIZADOR (UI) ---

st.title("üí° Assistente de An√°lise de Colabora√ß√µes")
st.markdown("Fa√ßa uma pergunta sobre o **hist√≥rico completo** de colabora√ß√µes e a IA ir√° gerar um insight para si.")
st.info("‚ÑπÔ∏è A aplica√ß√£o faz agora uma pr√©-an√°lise inteligente dos dados para respostas mais precisas.")

default_question = "Qual cidade teve mais colabora√ß√µes e qual o tipo de colabora√ß√£o mais comum?"
user_question = st.text_area("A sua pergunta:", value=default_question, height=100)

if st.button("Gerar Insight"):
    if not user_question:
        st.warning("Por favor, digite uma pergunta para an√°lise.")
    else:
        with st.spinner("A conectar-se √† base de dados e a buscar todo o hist√≥rico..."):
            dados_df = fetch_data_from_db()

        if dados_df is not None:
            if dados_df.empty:
                st.info("Nenhum registo encontrado na base de dados.")
            else:
                st.success(f"Dados carregados! {len(dados_df)} registos encontrados.")
                
                with st.spinner("O pr√©-analista inteligente est√° a resumir os dados e a contactar a IA..."):
                    insight = generate_insight_huggingface(user_question, dados_df)

                if insight:
                    st.subheader("An√°lise Gerada pela IA:")
                    st.markdown(insight)

